import numpy as np
import time
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

def calculate_information_loss(original_df, anonymized_df, qi_attributes):
    il = 0
    for col in qi_attributes:
        if original_df[col].dtype in [int, float, np.number]:
            orig_range = original_df[col].max() - original_df[col].min()
            anon_range = anonymized_df[col].apply(lambda x: float(str(x).split('-')[-1]) - float(str(x).split('-')[0]))
            il += anon_range.mean() / (orig_range + 1e-6)
        else:
            orig_unique = original_df[col].nunique()
            anon_unique = anonymized_df[col].nunique()
            il += 1 - (anon_unique / orig_unique)
    return il / len(qi_attributes)

def calculate_disclosure_risk(anonymized_df, k):
    groups = anonymized_df.groupby(anonymized_df.columns.tolist()).size()
    risky = sum(groups < k)
    risk = risky / len(anonymized_df)
    return risk

def evaluate_accuracy(model, X_test, y_test):
    y_pred = model.predict(X_test)
    return accuracy_score(y_test, y_pred)

def measure_runtime(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        runtime = end - start
        print(f"[Runtime] {func.__name__} took {runtime:.2f}s")
        return result, runtime
    return wrapper
