import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from scipy.stats import norm
from collections import defaultdict
import random

class DIVAAnonymizer:
    def __init__(self, k=5, rigorous=True):
        self.k = k
        self.rigorous = rigorous
        self.scaler = MinMaxScaler()
        self.label_encoders = {}
        self.taxon_trees = {}  # Placeholder for future taxonomy-based generalization

    def apply_ndf_filtering(self, df, column):
        mu = df[column].mean()
        sigma = df[column].std()
        threshold = 1 if self.rigorous else 2
        z_scores = (df[column] - mu) / sigma
        mask = (np.abs(z_scores) <= threshold)
        filtered = df[mask].copy()
        print(f"NDF Filtering: Reduced from {len(df)} to {len(filtered)} records.")
        return filtered

    def normalize_data(self, df, numeric_cols):
        df[numeric_cols] = self.scaler.fit_transform(df[numeric_cols])
        return df

    def encode_categorical(self, df, categorical_cols):
        for col in categorical_cols:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
            self.label_encoders[col] = le
        return df

    def cluster_data(self, df, quasi_identifiers):
        df = df.sample(frac=1).reset_index(drop=True)  # Shuffle
        clusters = [df.iloc[i:i+self.k] for i in range(0, len(df), self.k)]
        print(f"Clustering: Formed {len(clusters)} clusters.")
        return clusters

    def generalize_cluster(self, cluster, quasi_identifiers):
        generalized = {}
        for col in quasi_identifiers:
            if cluster[col].dtype.kind in 'iufc':  # numerical
                generalized[col] = [cluster[col].min(), cluster[col].max()]
            else:  # categorical - generalize to most common
                generalized[col] = cluster[col].mode()[0]
        return generalized

    def generalize_clusters(self, clusters, quasi_identifiers):
        generalized_data = []
        for cluster in clusters:
            generalized_cluster = self.generalize_cluster(cluster, quasi_identifiers)
            for _, row in cluster.iterrows():
                new_row = row.copy()
                for col in quasi_identifiers:
                    gen_val = generalized_cluster[col]
                    if isinstance(gen_val, list):
                        new_row[col] = f"{gen_val[0]}-{gen_val[1]}"
                    else:
                        new_row[col] = gen_val
                generalized_data.append(new_row)
        return pd.DataFrame(generalized_data)

    def integrate_clusters(self, generalized_df):
        return generalized_df

    def compute_information_loss(self, original_df, anonymized_df, quasi_identifiers):
        loss = 0
        for col in quasi_identifiers:
            if anonymized_df[col].dtype == object:
                loss += anonymized_df[col].apply(lambda x: 1 if "-" in str(x) else 0).sum()
        info_loss_ratio = loss / len(anonymized_df)
        print(f"Information Loss: {info_loss_ratio:.4f}")
        return info_loss_ratio

    def run_full_anonymization(self, df, target_col, quasi_identifiers, sensitive_cols):
        df_filtered = self.apply_ndf_filtering(df, target_col)
        df_encoded = self.encode_categorical(df_filtered.copy(), quasi_identifiers)
        df_normalized = self.normalize_data(df_encoded, [col for col in df_encoded.columns if df_encoded[col].dtype != object])

        clusters = self.cluster_data(df_normalized, quasi_identifiers)
        generalized_df = self.generalize_clusters(clusters, quasi_identifiers)
        integrated_df = self.integrate_clusters(generalized_df)

        info_loss = self.compute_information_loss(df_filtered, integrated_df, quasi_identifiers)
        return integrated_df, info_loss
